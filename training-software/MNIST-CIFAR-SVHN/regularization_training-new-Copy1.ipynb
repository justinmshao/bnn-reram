{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167682b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Keras Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "#Utility\n",
    "# from utils import binarize\n",
    "sys.path.insert(0, '..')\n",
    "from binarization_utils import *\n",
    "# from model_architectures import get_model\n",
    "\n",
    "#Probability\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51147cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c08114",
   "metadata": {},
   "outputs": [],
   "source": [
    "training=True\n",
    "dataset='CIFAR-10'\n",
    "# Train=True\n",
    "# Evaluate=False\n",
    "\n",
    "batch_norm_eps=1e-4\n",
    "batch_norm_alpha=0.1\n",
    "\n",
    "# for loading SVHN dataset\n",
    "def load_svhn(path_to_dataset):\n",
    "\timport scipy.io as sio\n",
    "\ttrain=sio.loadmat(path_to_dataset+'/train.mat')\n",
    "\ttest=sio.loadmat(path_to_dataset+'/test.mat')\n",
    "\textra=sio.loadmat(path_to_dataset+'/extra.mat')\n",
    "\tX_train=np.transpose(train['X'],[3,0,1,2])\n",
    "\ty_train=train['y']-1\n",
    "\n",
    "\tX_test=np.transpose(test['X'],[3,0,1,2])\n",
    "\ty_test=test['y']-1\n",
    "\n",
    "\tX_extra=np.transpose(extra['X'],[3,0,1,2])\n",
    "\ty_extra=extra['y']-1\n",
    "\n",
    "\tX_train=np.concatenate((X_train,X_extra),axis=0)\n",
    "\ty_train=np.concatenate((y_train,y_extra),axis=0)\n",
    "\n",
    "\treturn (X_train,y_train),(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e63703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset==\"MNIST\":\n",
    "\t(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\t# convert class vectors to binary class matrices\n",
    "\tX_train = X_train.reshape(-1,784)\n",
    "\tX_test = X_test.reshape(-1,784)\n",
    "\tuse_generator=False\n",
    "elif dataset==\"CIFAR-10\":\n",
    "\tuse_generator=True\n",
    "\t(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "elif dataset==\"SVHN\":\n",
    "\tuse_generator=True\n",
    "\t(X_train, y_train), (X_test, y_test) = load_svhn('svhn_data')\n",
    "else:\n",
    "\traise(\"dataset should be one of the following: [MNIST, CIFAR-10, SVHN].\")\n",
    "\n",
    "    \n",
    "#Restructure Data\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "# Y_train = to_categorical(y_train, 10)\n",
    "# Y_test = to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "\n",
    "#Train Val Split\n",
    "X_train,X_val,y_train, y_val = train_test_split(X_train,y_train,test_size = 0.10)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(y_train.shape[0], 'train samples')\n",
    "\n",
    "print(X_val.shape[0], 'val samples')\n",
    "print(y_val.shape[0], 'val samples')\n",
    "\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(y_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1068183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sign_layer(Layer):\n",
    "    def __init__(self, levels=1,**kwargs):\n",
    "        self.levels=levels\n",
    "        super(Sign_layer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        ars=np.arange(self.levels)+1.0\n",
    "        ars=ars[::-1]\n",
    "        means=ars/np.sum(ars)\n",
    "        self.means=[K.variable(m) for m in means]\n",
    "        self._trainable_weights = self.means\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        resid = x\n",
    "        out_bin=0\n",
    "        for l in range(self.levels):\n",
    "            out=binarize(resid)*(K.abs(self.means[l]))\n",
    "#             print(out)\n",
    "            out_bin=out_bin+out\n",
    "            resid=resid-out\n",
    "        return out_bin\n",
    "    \n",
    "        # the following lines were an idea to implement flips using tensor operations\n",
    "        '''positive_mask = tf.cast(out_bin > 0, tf.float32)\n",
    "        negative_mask = tf.cast(out_bin < 0, tf.float32)\n",
    "        \n",
    "        positive_flips = tf.random.uniform(out_bin.shape) < p[1]\n",
    "        positives = tf.math.multiply(positive_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[1], tf.float32) - 1))\n",
    "        negatives = tf.math.multiply(negative_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[0], tf.float32) - 1))\n",
    "        return'''\n",
    "\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return input_shape\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    def set_means(self,X):\n",
    "        means=np.zeros((self.levels))\n",
    "        means[0]=1\n",
    "        resid=np.clip(X,-1,1)\n",
    "        approx=0\n",
    "        for l in range(self.levels):\n",
    "            m=np.mean(np.absolute(resid))\n",
    "            out=np.sign(resid)*m\n",
    "            approx=approx+out\n",
    "            resid=resid-out\n",
    "            means[l]=m\n",
    "            err=np.mean((approx-np.clip(X,-1,1))**2)\n",
    "\n",
    "        means=means/np.sum(means)\n",
    "        sess=K.get_session()\n",
    "        sess.run(self.means.assign(means))\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'levels': self.levels\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class binary_dense_error_var(Layer):\n",
    "    def __init__(self,n_in,n_out,error_type,error_dict,**kwargs):\n",
    "        self.n_in=n_in\n",
    "        self.n_out=n_out\n",
    "        self.error_type=error_type\n",
    "        self.error_dict= error_dict\n",
    "        assert self.error_type in [\"NoError\", \"Flip\",\"InnerDot\"], \"error_type of {0} is invalid\".format(error_type)\n",
    "        super(binary_dense_error_var,self).__init__(**kwargs)\n",
    "        \n",
    "        if(error_type == \"InnerDot\"):\n",
    "            max_pop = self.n_in\n",
    "            self.uh = self.error_dict[\"uh\"]\n",
    "            self.ul = self.error_dict[\"ul\"]\n",
    "            self.sigl = self.error_dict[\"sigl\"]\n",
    "            self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "            #Not Used, Mainly used for information\n",
    "            means = [m * self.ul + (max_pop - m) * self.uh for m in range(0, max_pop+1)]\n",
    "            stdvs = [np.sqrt(m * (self.sigl**2) + (max_pop - m)*(self.sigh**2)) for m in range(0, max_pop+1)]\n",
    "            self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "    \n",
    "        elif(error_type == \"Flip\"):\n",
    "            self.p = self.error_dict[\"p\"]\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        stdv=1/np.sqrt(self.n_in)\n",
    "        w = np.random.normal(loc=0.0, scale=stdv,size=[self.n_in,self.n_out]).astype(np.float32)\n",
    "        self.w=K.variable(w)\n",
    "        self.gamma_w=K.variable(1.0)\n",
    "        self._trainable_weights=[self.w,self.gamma_w]\n",
    "        \n",
    "        \n",
    "#         if(error_type == \"InnerDot\"):\n",
    "#             max_pop = self.n_in+1\n",
    "#             self.uh = self.error_dict[\"uh\"]\n",
    "#             self.ul = self.error_dict[\"ul\"]\n",
    "#             self.sigl = self.error_dict[\"sigl\"]\n",
    "#             self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             means = [m * ul + (max_pop - 1 - m) * uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (sigl**2) + (max_pop - 1 - m)*(sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "            \n",
    "    def call(self, x,training=None):\n",
    "        \n",
    "        #Designed with Batch norm in mind, Binarization done after. \n",
    "        if(self.error_type == \"NoError\"):\n",
    "            self.clamped_w=binarize(self.w)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"Flip\"):\n",
    "            self.clamped_w=binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"InnerDot\"):\n",
    "            \n",
    "            if training:\n",
    "                \n",
    "                #Perfrom error model on inference\n",
    "                #No Error Model first\n",
    "                self.clamped_w=binarize(self.w)\n",
    "                \n",
    "                self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "                \n",
    "                # Make Model\n",
    "                self.clamped_x=binarize(x)\n",
    "                gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "                #self.prod = K.dot(x,self.clamped_w)\n",
    "                self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "                #Add Error \n",
    "                self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                     scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "                \n",
    "                self.samps = self.dist.sample(1)\n",
    "                self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "        \n",
    "                #Use Histogram for binning\n",
    "                self.step = self.ul - self.uh\n",
    "                self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "                self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "                self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "                \n",
    "                #Gradient not using binning\n",
    "                return  self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "\n",
    "            else:\n",
    "                #Perfrom error model on inference\n",
    "                #No Error Model first\n",
    "                self.clamped_w=binarize(self.w)\n",
    "                self.clamped_x=binarize(x)\n",
    "                gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "                #self.prod = K.dot(x,self.clamped_w)\n",
    "                self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "                #Add Error \n",
    "                self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                     scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "                \n",
    "\n",
    "                self.samps = self.dist.sample(1)\n",
    "                self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "        \n",
    "                #Use Histogram for binning\n",
    "                self.step = self.ul - self.uh\n",
    "                self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "                self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "                self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "            \n",
    "                return self.out\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_in': self.n_in,\n",
    "            'n_out': self.n_out,\n",
    "            'error_type': self.error_type,\n",
    "            'error_dict': self.error_dict\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_batchnorm_sign(Layer):\n",
    "    def __init__(self,nfilters,ch_in,k,padding,error_type,error_dict,strides=(1,1),batch_norm_params={\"eps\" : 1e-4, \"alpha\" : 0.1},reg_params={\"type\" : None, \"b\" : 32, \"lambda\" : 0.0001, \"alpha\" : 1},**kwargs):\n",
    "        self.nfilters=nfilters\n",
    "        self.ch_in=ch_in\n",
    "        self.k=k\n",
    "        self.padding=padding\n",
    "        self.strides=strides\n",
    "        self.error_type=error_type\n",
    "        self.error_dict= error_dict\n",
    "        self.batch_norm_eps = batch_norm_params[\"eps\"]\n",
    "        self.batch_norm_alpha = batch_norm_params[\"alpha\"]\n",
    "        \n",
    "        # regularization hyperparameters\n",
    "        self.loss_type = reg_params[\"type\"]\n",
    "        # desired bit error tolerance\n",
    "        self.b = reg_params[\"b\"]\n",
    "        self.reg_lambda = reg_params[\"lambda\"]\n",
    "        self.alpha = reg_params[\"alpha\"]\n",
    "        \n",
    "        assert self.error_type in [\"NoError\", \"Flip\",\"InnerDot\"], \"error_type of {0} is invalid\".format(error_type)\n",
    "        \n",
    "\n",
    "        super(conv_batchnorm_sign,self).__init__(**kwargs)\n",
    "        \n",
    "        if(error_type == \"InnerDot\"):\n",
    "            self.uh = self.error_dict[\"uh\"]\n",
    "            self.ul = self.error_dict[\"ul\"]\n",
    "            self.sigl = self.error_dict[\"sigl\"]\n",
    "            self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             #Not Used, Mainly used for information\n",
    "#             means = [m * self.ul + (max_pop - m) * self.uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (self.sigl**2) + (max_pop - m)*(self.sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "    \n",
    "        elif(error_type == \"Flip\"):\n",
    "            self.p = self.error_dict[\"p\"]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        stdv=1/np.sqrt(self.k*self.k*self.ch_in)\n",
    "        w = np.random.normal(loc=0.0, scale=stdv,size=[self.k,self.k,self.ch_in,self.nfilters]).astype(np.float32)\n",
    "        if tensorflow.keras.backend.backend()==\"mxnet\":\n",
    "            w=w.transpose(3,2,0,1)\n",
    "        self.w=K.variable(w)\n",
    "        self.gamma_w=K.variable(1.0)\n",
    "#         self._trainable_weights=[self.w,self.gamma_w]\n",
    "        \n",
    "        self.max_pop = self.k*self.k*self.ch_in\n",
    "\n",
    "        # used for regularization\n",
    "        self.uh = self.error_dict[\"uh\"]\n",
    "        self.ul = self.error_dict[\"ul\"]\n",
    "        self.sigl = self.error_dict[\"sigl\"]\n",
    "        self.sigh = self.error_dict[\"sigh\"]\n",
    "        \n",
    "        # batchnorm and sign activation\n",
    "        self.batchnorm_layer = BatchNormalization(axis=-1, momentum=self.batch_norm_alpha, epsilon=self.batch_norm_eps)\n",
    "        self.sign_layer = Sign_layer(levels=1)\n",
    "        \n",
    "#         self.batchnorm_layer.build(input_shape=(None, input_shape[1]-(self.k-1), input_shape[2]-(self.k-1), self.nfilters))\n",
    "#         self.sign_layer.build(input_shape)\n",
    "\n",
    "        self._trainable_weights=[self.w, self.gamma_w] + self.batchnorm_layer._trainable_weights + self.sign_layer._trainable_weights\n",
    "\n",
    "    def call(self, x,mask=None, training=None):\n",
    "        \n",
    "        if(self.error_type == \"NoError\"):\n",
    "            constraint_gamma=K.abs(self.gamma_w)#K.clip(self.gamma_w,0.01,10)\n",
    "            self.clamped_w=constraint_gamma*binarize(self.w)\n",
    "            self.out=K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides )\n",
    "            self.output_dim=self.out.get_shape().as_list()\n",
    "#             self.clamped_w=binarize(self.w)\n",
    "#             self.prod=K.abs(self.gamma_w)*K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides )\n",
    "#             self.clamped_x=binarize(x)\n",
    "#             gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "#             self.pop = (K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides)+self.max_pop)/2\n",
    "#             self.step = self.ul - self.uh\n",
    "#             self.edges = np.arange(self.max_pop*self.uh-self.step/2, self.max_pop*self.ul+self.step, self.step,dtype=np.float32)\n",
    "#             self.bins= tfp.stats.find_bins(self.pop * self.ul + (self.max_pop - self.pop) * self.uh, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "#             self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.max_pop)\n",
    "#             self.out = self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "        elif(self.error_type == \"Flip\"):\n",
    "            constraint_gamma=K.abs(self.gamma_w)#K.clip(self.gamma,0.01,10)\n",
    "            self.clamped_w=constraint_gamma*binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "            self.out=K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides )\n",
    "            self.output_dim=self.out.get_shape().as_list() \n",
    "\n",
    "        elif(self.error_type == \"InnerDot\"):\n",
    "            \n",
    "\n",
    "\n",
    "            #Perfrom error model on inference\n",
    "            #No Error Model first\n",
    "            self.clamped_w=binarize(self.w)\n",
    "\n",
    "            #self.prod=K.abs(self.gamma_w)*K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides )\n",
    "            self.prod=K.abs(self.gamma_w)*K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides )\n",
    "\n",
    "            # Make Model\n",
    "            self.clamped_x=binarize(x)\n",
    "            gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "            #self.prod = K.dot(x,self.clamped_w)\n",
    "            #self.pop = (K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides)+self.max_pop)/2\n",
    "            self.pop = (K.conv2d(x, kernel=self.clamped_w, padding=self.padding,strides=self.strides)+self.max_pop)/2\n",
    "\n",
    "            #Add Error \n",
    "            self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.max_pop - self.pop) * self.uh,\n",
    "                                                 scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.max_pop - self.pop)*(self.sigh**2)))\n",
    "\n",
    "            self.samps = self.dist.sample(1)\n",
    "            self.samps = tf.squeeze(self.samps, axis = 0)                \n",
    "\n",
    "            #Use Histogram for binning\n",
    "            self.step = self.ul - self.uh\n",
    "            self.edges = np.arange(self.max_pop*self.uh-self.step/2, self.max_pop*self.ul+self.step, self.step,dtype=np.float32)\n",
    "            self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "            self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.max_pop)\n",
    "            \n",
    "            self.out = self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "\n",
    "        # regulariztion\n",
    "        if training:\n",
    "            \n",
    "\n",
    "            # regularization\n",
    "            # threshold regularization\n",
    "            params = self.batchnorm_layer.weights\n",
    "            print(params)\n",
    "            gamma = params[0]\n",
    "            beta = params[1]\n",
    "            mean = params[2]\n",
    "            var = params[3]\n",
    "            T = mean - tf.math.divide(tf.math.sqrt(var),gamma)*beta\n",
    "            \n",
    "            if self.loss_type == \"uniform\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(tf.nn.relu(self.b - tf.math.abs(self.out - T))))\n",
    "            elif self.loss_type == \"exp\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(-1*tf.math.exp(self.alpha*(2*T - self.max_pop))*tf.math.abs(self.out - T)))\n",
    "            elif self.loss_type == \"inverse\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(tf.math.reciprocal(2*T - self.max_pop)*(self.b - tf.math.abs(self.out - T))))\n",
    "                \n",
    "                \n",
    "        self.out = self.batchnorm_layer(self.out)\n",
    "        self.out = self.sign_layer(self.out)\n",
    "        return self.out\n",
    "        \n",
    "    def  get_output_shape_for(self,input_shape):\n",
    "        return (input_shape[0], self.output_dim[1],self.output_dim[2],self.output_dim[3])\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0], self.output_dim[1],self.output_dim[2],self.output_dim[3])\n",
    "    \n",
    "class dense_batchnorm_sign(Layer):\n",
    "    def __init__(self,n_in,n_out,error_type,error_dict,batch_norm_params={\"eps\" : 1e-4, \"alpha\" : 0.1},reg_params={\"type\" : None, \"b\" : 32, \"lambda\" : 0.0001, \"alpha\" : 1},**kwargs):\n",
    "        self.n_in=n_in\n",
    "        self.n_out=n_out\n",
    "        self.error_type=error_type\n",
    "        self.error_dict= error_dict\n",
    "        self.batch_norm_eps = batch_norm_params[\"eps\"]\n",
    "        self.batch_norm_alpha = batch_norm_params[\"alpha\"]\n",
    "        \n",
    "        # regularization hyperparameters\n",
    "        self.loss_type = reg_params[\"type\"]\n",
    "        # desired bit error tolerance\n",
    "        self.b = reg_params[\"b\"]\n",
    "        self.reg_lambda = reg_params[\"lambda\"]\n",
    "        self.alpha = reg_params[\"alpha\"]\n",
    "        \n",
    "        assert self.error_type in [\"NoError\", \"Flip\",\"InnerDot\"], \"error_type of {0} is invalid\".format(error_type)\n",
    "        super(dense_batchnorm_sign,self).__init__(**kwargs)\n",
    "        \n",
    "        self.max_pop = self.n_in\n",
    "        if(error_type == \"InnerDot\"):\n",
    "            max_pop = self.n_in\n",
    "            self.uh = self.error_dict[\"uh\"]\n",
    "            self.ul = self.error_dict[\"ul\"]\n",
    "            self.sigl = self.error_dict[\"sigl\"]\n",
    "            self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             #Not Used, Mainly used for information\n",
    "#             means = [m * self.ul + (max_pop - m) * self.uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (self.sigl**2) + (max_pop - m)*(self.sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "    \n",
    "        elif(error_type == \"Flip\"):\n",
    "            self.p = self.error_dict[\"p\"]\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        stdv=1/np.sqrt(self.n_in)\n",
    "        w = np.random.normal(loc=0.0, scale=stdv,size=[self.n_in,self.n_out]).astype(np.float32)\n",
    "        self.w=K.variable(w)\n",
    "        self.gamma_w=K.variable(1.0)\n",
    "        \n",
    "        # batchnorm and sign activation\n",
    "        self.batchnorm_layer = BatchNormalization(axis=-1, momentum=self.batch_norm_alpha, epsilon=self.batch_norm_eps)\n",
    "        self.sign_layer = Sign_layer(levels=1)\n",
    "        \n",
    "        self._trainable_weights=[self.w,self.gamma_w] + self.batchnorm_layer._trainable_weights + self.sign_layer._trainable_weights\n",
    "        \n",
    "        # used for regularization\n",
    "        self.uh = self.error_dict[\"uh\"]\n",
    "        self.ul = self.error_dict[\"ul\"]\n",
    "        self.sigl = self.error_dict[\"sigl\"]\n",
    "        self.sigh = self.error_dict[\"sigh\"]\n",
    "        \n",
    "#         if(error_type == \"InnerDot\"):\n",
    "#             max_pop = self.n_in+1\n",
    "#             self.uh = self.error_dict[\"uh\"]\n",
    "#             self.ul = self.error_dict[\"ul\"]\n",
    "#             self.sigl = self.error_dict[\"sigl\"]\n",
    "#             self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             means = [m * ul + (max_pop - 1 - m) * uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (sigl**2) + (max_pop - 1 - m)*(sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "            \n",
    "    def call(self, x,training=None):\n",
    "        \n",
    "        #Designed with Batch norm in mind, Binarization done after. \n",
    "        if(self.error_type == \"NoError\"):\n",
    "            self.clamped_w=binarize(self.w)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "\n",
    "#             self.clamped_w=binarize(self.w)\n",
    "#             self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "#             self.clamped_x=binarize(x)\n",
    "#             gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "#             self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "#             self.out = K.abs(self.gamma_w)*gamma_x*(2*(self.pop * self.ul + (self.n_in - self.pop) * self.uh)-self.n_in)         \n",
    "#             self.out = self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "#             self.clamped_w=binarize(self.w)\n",
    "\n",
    "#             self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "#             self.clamped_x=binarize(x)\n",
    "#             gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "#             self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "#             #Use Histogram for binning\n",
    "#             self.step = self.ul - self.uh\n",
    "#             self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "#             self.bins= tfp.stats.find_bins(self.pop * self.ul + (self.n_in - self.pop) * self.uh, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "#             self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "#             self.out = self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "        elif(self.error_type == \"Flip\"):\n",
    "            self.clamped_w=binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "        elif(self.error_type == \"InnerDot\"):\n",
    "                \n",
    "            #Perfrom error model on inference\n",
    "            #No Error Model first\n",
    "            self.clamped_w=binarize(self.w)\n",
    "\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "\n",
    "            # Make Model\n",
    "            self.clamped_x=binarize(x)\n",
    "            gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "            #self.prod = K.dot(x,self.clamped_w)\n",
    "            self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "            #Add Error \n",
    "            self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                 scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "\n",
    "            self.samps = self.dist.sample(1)\n",
    "            self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "\n",
    "            #Use Histogram for binning\n",
    "            self.step = self.ul - self.uh\n",
    "            self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "            self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "            self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "            self.out = self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "\n",
    "        # regulariztion\n",
    "        if training:\n",
    "            \n",
    "\n",
    "            # regularization\n",
    "            # threshold regularization\n",
    "            params = self.batchnorm_layer.weights\n",
    "            print(params)\n",
    "            gamma = params[0]\n",
    "            beta = params[1]\n",
    "            mean = params[2]\n",
    "            var = params[3]\n",
    "            T = mean - tf.math.divide(tf.math.sqrt(var),gamma)*beta\n",
    "\n",
    "            if self.loss_type == \"uniform\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(tf.nn.relu(self.b - tf.math.abs(self.out - T))))\n",
    "            elif self.loss_type == \"exp\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(-1*tf.math.exp(self.alpha*(2*T - self.max_pop))*tf.math.abs(self.out - T)))\n",
    "            elif self.loss_type == \"inverse\":\n",
    "                self.add_loss(self.reg_lambda*tf.reduce_mean(tf.math.reciprocal(2*T - self.max_pop)*(self.b - tf.math.abs(self.out - T))))\n",
    "          \n",
    "            \n",
    "        self.out = self.batchnorm_layer(self.out)\n",
    "        self.out = self.sign_layer(self.out)\n",
    "        return self.out\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (self.n_in, self.n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6be451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CIFAR10_SVHN_model(error_type, p = 0.1, ul = 1, sigl = 0.01, uh=0.01, sigh = 0.01, reg_params = {\"type\" : \"exp\", \"b\" : 64, \"lambda\" : 0.0001, \"alpha\" : 1}):\n",
    "    if(error_type == 0):\n",
    "        #No Error\n",
    "        error_type = \"NoError\"\n",
    "        #error_dict = {}\n",
    "        # for regularization\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "    elif(error_type == 1):\n",
    "        #Flip Error\n",
    "        error_type = \"Flip\"\n",
    "        error_dict = {\"p\": p }\n",
    "    elif(error_type == 2):\n",
    "        #InnerDot error \n",
    "        error_type = \"InnerDot\"\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}        \n",
    "    \n",
    "    #Model Def\n",
    "    model=Sequential()\n",
    "    model.add(conv_batchnorm_sign(nfilters=64,ch_in=3,k=3,padding='valid',input_shape=[32,32,3],error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(conv_batchnorm_sign(nfilters=64,ch_in=64,k=3,padding='valid',error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "    \n",
    "    model.add(conv_batchnorm_sign(nfilters=128,ch_in=64,k=3,padding='valid',error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(conv_batchnorm_sign(nfilters=128,ch_in=128,k=3,padding='valid',error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(conv_batchnorm_sign(nfilters=256,ch_in=128,k=3,padding='valid',error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(conv_batchnorm_sign(nfilters=256,ch_in=256,k=3,padding='valid',error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "\n",
    "    model.add(my_flat())\n",
    "\n",
    "    model.add(dense_batchnorm_sign(n_in=int(model.output.get_shape()[1]),n_out=512,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(dense_batchnorm_sign(n_in=int(model.output.get_shape()[1]),n_out=512,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(binary_dense_error_var(n_in=int(model.output.get_shape()[1]),n_out=10,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the following is a workaround so that the model weights can be saved\n",
    "    # https://github.com/tensorflow/tensorflow/issues/46871\n",
    "    j = 0\n",
    "    for w in model.weights:\n",
    "        w._handle_name = 'model_' + str(j) + w.name\n",
    "        j = j + 1\n",
    "        \n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_MNIST_model(error_type, p = 0.1, ul = 1, sigl = 0.01, uh=0.01, sigh = 0.01, reg_params = {\"type\" : \"exp\", \"b\" : 64, \"lambda\" : 0.0001, \"alpha\" : 1}):\n",
    "    if(error_type == 0):\n",
    "        #No Error\n",
    "        error_type = \"NoError\"\n",
    "        #error_dict = {}\n",
    "        # for regularization\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "    elif(error_type == 1):\n",
    "        #Flip Error\n",
    "        error_type = \"Flip\"\n",
    "        error_dict = {\"p\": p }\n",
    "    elif(error_type == 2):\n",
    "        #InnerDot error \n",
    "        error_type = \"InnerDot\"\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}   \n",
    "    model=Sequential()\n",
    "    model.add(dense_batchnorm_sign(n_in=784,n_out=256,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(dense_batchnorm_sign(n_in=int(model.output.get_shape()[1]),n_out=256,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(dense_batchnorm_sign(n_in=int(model.output.get_shape()[1]),n_out=256,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(dense_batchnorm_sign(n_in=int(model.output.get_shape()[1]),n_out=256,error_type = error_type, error_dict = error_dict, reg_params = reg_params))\n",
    "    model.add(binary_dense_error(n_in=int(model.output.get_shape()[1]),n_out=10,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Construction\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "batch_norm_eps=1e-4\n",
    "batch_norm_alpha=0.1\n",
    "batch_norm_params = {\"eps\" : batch_norm_eps, \"alpha\" : batch_norm_alpha}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8117e",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "error_type = 0 # training with no error\n",
    "\n",
    "#Flip Probability\n",
    "p=0.1\n",
    "\n",
    "#Regularization params\n",
    "reg_params = {\"type\" : \"uniform\", \"b\" : 64, \"lambda\" : 0.0001, \"alpha\" : 1}\n",
    "\n",
    "#InnerProd params\n",
    "ul = 1 #l-low means bits match\n",
    "uh = 0.1\n",
    "sigl = 0.01\n",
    "sigh = 0.2\n",
    "\n",
    "#Make Model\n",
    "model = make_CIFAR10_SVHN_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh, reg_params = reg_params)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ab77e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training Script\n",
    "training=True\n",
    "    \n",
    "#Training parameters\n",
    "batch_size=50\n",
    "epochs=50\n",
    "error_type=0\n",
    "\n",
    "#Makes Model Name\n",
    "if(error_type == 0):\n",
    "    #No Error\n",
    "    model_name = \"error_{0}\".format(error_type)\n",
    "elif(error_type == 1):\n",
    "    model_name = \"error_{0}_p_{1}\".format(error_type,p)\n",
    "elif(error_type == 2):\n",
    "    #InnerDot error \n",
    "    model_name = \"error_{0}_ul_{1}_uh_{2}_sigl_{3}_sigh_{4}\".format(error_type,ul,uh,sigl,sigh)\n",
    "\n",
    "model_name = 'uniform'\n",
    "#model_name = 'uniform'\n",
    "if not(os.path.exists('models')):\n",
    "    os.mkdir('models')\n",
    "if not(os.path.exists('models/'+model_name)):\n",
    "    os.mkdir('models/'+model_name)\n",
    "    \n",
    "    \n",
    "# #gather all binary dense and binary convolution layers:\n",
    "# binary_layers=[]\n",
    "# for l in model.layers:\n",
    "#     if isinstance(l,binary_dense) or isinstance(l,binary_conv):\n",
    "#         binary_layers.append(l)\n",
    "\n",
    "#Train\n",
    "lr=0.01\n",
    "opt = keras.optimizers.Adam(learning_rate=lr,decay=1e-6)#SGD(lr=lr,momentum=0.9,decay=1e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "\n",
    "\n",
    "weights_path='models/'+model_name+'/'+model_name+'.h5'\n",
    "cback=keras.callbacks.ModelCheckpoint(weights_path, monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n",
    "\n",
    "\n",
    "#Training\n",
    "if keras.__version__[0]=='2':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,epochs=epochs,callbacks=[cback])\n",
    "if keras.__version__[0]=='1':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,nb_epoch=epochs,callbacks=[cback])\n",
    "\n",
    "#Save history    \n",
    "dic={'hard':history.history}\n",
    "foo=open('models/'+model_name+'.pkl','wb')\n",
    "pickle.dump(dic,foo)\n",
    "foo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb271c57",
   "metadata": {},
   "source": [
    "# Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b30a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "training=False\n",
    "\n",
    "#Test Based on Error Type\n",
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "train_error_type = 0 #can be 0 or 1 or 2\n",
    "test_error_type = 2 # can be 0 1 or 2\n",
    "train_flip_probability = 0.1\n",
    "\n",
    "#Training InnerProd params\n",
    "train_ul = 1 #l-low means bits match\n",
    "train_uh = 0.1\n",
    "train_sigl = 0.01\n",
    "train_sigh = 0.2\n",
    "\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "\n",
    "# Test Parameters  \n",
    "#InnerProd params\n",
    "ul = 1\n",
    "uh = 0.1\n",
    "sigl = 0.01\n",
    "sigh = 0.2\n",
    "\n",
    "#Makes Model Name\n",
    "if(train_error_type == 0):\n",
    "    #No Error\n",
    "    model_name = \"error_{0}\".format(train_error_type)\n",
    "elif(train_error_type == 1):\n",
    "    model_name = \"error_{0}_p_{1}\".format(train_error_type,train_flip_probability)\n",
    "elif(train_error_type == 2):\n",
    "    #InnerDot error \n",
    "    model_name = \"error_{0}_ul_{1}_uh_{2}_sigl_{3}_sigh_{4}\".format(train_error_type,train_ul,train_uh,train_sigl,train_sigh)    \n",
    "    \n",
    "model_name = 'uniform'\n",
    "weights_path = 'models/'+model_name+'/'+model_name+'.h5' #To Specify  \n",
    "\n",
    "#weights_path = \"\" #To Specify\n",
    "\n",
    "if(test_error_type == 0):\n",
    "    #No Error\n",
    "    model=make_CIFAR10_SVHN_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "    model.compile()\n",
    "\n",
    "    #Load Weights\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    score=model.evaluate(X_test,y_test,verbose=0)\n",
    "    print(\"No Error, test loss was %0.4f, test accuracy was %0.4f\"%(score[0],score[1]))\n",
    "\n",
    "elif(test_error_type == 1):\n",
    "    #Flip Error\n",
    "    probs = np.logspace(-4, -1, 20)\n",
    "    \n",
    "    acc_1 = []\n",
    "    for p in probs:\n",
    "        \n",
    "        model=make_MNIST_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "        model.compile()\n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        #print(\"With p= %0.4f, test loss was %0.4f, test accuracy was %0.4f\" %(p,score[0],score[1]))\n",
    "        print('%0.4f' %(score[1]))\n",
    "        acc_1.append(score[1])\n",
    "    \n",
    "elif(test_error_type == 2):\n",
    "    #InnerDot Error\n",
    "    error_type = \"InnerDot\"\n",
    "    \n",
    "    ratio_l_h = np.linspace(1,80,20)\n",
    "    acc_2 = []\n",
    "    for ratio in ratio_l_h:\n",
    "        print(ratio)\n",
    "    for ratio in ratio_l_h:\n",
    "        \n",
    "        model = make_CIFAR10_SVHN_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = ratio*sigl )\n",
    "        \n",
    "        #model.compile()\n",
    "        lr=0.01\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr,decay=1e-6)#SGD(lr=lr,momentum=0.9,decay=1e-5)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy']) \n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        #print(\"With ratio %0.4f, test loss was %0.4f, test accuracy was %0.4f\"%(ratio,score[0],score[1]))\n",
    "        print('%0.4f' %(score[1]))\n",
    "        #print(score)\n",
    "        acc_2.append(score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = np.linspace(1,80,20)\n",
    "c10_accuracies = [0.6668, 0.6753, 0.6895, 0.6986, 0.7043, 0.7023, 0.6918, 0.6748, 0.6527, 0.6182, 0.5813, 0.5423, 0.4911, 0.4535, 0.4120, 0.3623, 0.3227, 0.2917, 0.2578, 0.2321]\n",
    "threshold_reg = [0.6984, 0.7008, 0.6980, 0.6859, 0.6761, 0.6576, 0.6455, 0.6143, 0.5939, 0.5615, 0.5322, 0.5007, 0.4622, 0.4322, 0.3969, 0.3761, 0.3416, 0.3148, 0.2881, 0.2646]\n",
    "pop_reg = [0.6888, 0.6918, 0.6923, 0.6779, 0.6643, 0.6382, 0.6024, 0.5622, 0.5220, 0.4694, 0.4268, 0.3877, 0.3391, 0.3088, 0.2835, 0.2535, 0.2404, 0.2175, 0.2030, 0.1937];\n",
    "#inverse_threshold_weighted_reg = [0.6908, 0.6936, 0.6894, 0.6781, 0.6650, 0.6414, 0.6152, 0.5820, 0.5482, 0.5102, 0.4762, 0.4335, 0.3929, 0.3605, 0.3390, 0.3023, 0.2813, 0.2569, 0.2387, 0.2247]\n",
    "inverse_threshold_weighted_reg = [0.6929, 0.7010, 0.7083, 0.7108, 0.7123, 0.7003, 0.6850, 0.6579, 0.6188, 0.5749, 0.5279, 0.4767, 0.4219, 0.3758, 0.3336, 0.2918, 0.2586, 0.2273, 0.2069, 0.1871]\n",
    "#exp_threshold_weighted_reg = [0.6868, 0.6864, 0.6814, 0.6657, 0.6530, 0.6230, 0.5959, 0.5540, 0.5046, 0.4592, 0.4086, 0.3646, 0.3231, 0.2927, 0.2644, 0.2364, 0.2177, 0.2048, 0.1938, 0.1832]\n",
    "exp_threshold_weighted_reg = [0.6314, 0.6374, 0.6464, 0.6595, 0.6632, 0.6525, 0.6454, 0.6185, 0.5917, 0.5533, 0.5074, 0.4629, 0.4145, 0.3754, 0.3350, 0.3092, 0.2749, 0.2511, 0.2296, 0.2098]\n",
    "plt.plot(ratios,c10_accuracies,label='2 train 2 test')\n",
    "plt.plot(ratios,threshold_reg, label='threshold_reg with threshold = maxpop/2')\n",
    "plt.plot(ratios,pop_reg, label='maxpop - pop regularization')\n",
    "plt.plot(ratios,inverse_threshold_weighted_reg, label='1/T weight regularization')\n",
    "plt.plot(ratios,exp_threshold_weighted_reg, label='exp(-T) weight regularization')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies1 = [0.6668, 0.6753, 0.6895, 0.6986, 0.7043, 0.7023, 0.6918, 0.6748, 0.6527, 0.6182, 0.5813, 0.5423, 0.4911, 0.4535, 0.4120, 0.3623, 0.3227, 0.2917, 0.2578, 0.2321]\n",
    "accuracies2 = [0.6426, 0.6535, 0.6640, 0.6676, 0.6748, 0.6687, 0.6548, 0.6212, 0.5848, 0.5379, 0.4815, 0.4292, 0.3776, 0.3249, 0.2797, 0.2431, 0.2110, 0.1841, 0.1727, 0.1625]\n",
    "plt.plot(ratios,accuracies1)\n",
    "plt.plot(ratios,accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform=[0.4361, 0.4351, 0.4291, 0.4088, 0.3812, 0.3570, 0.3269, 0.2957, 0.2669, 0.2379, 0.2059, 0.1830, 0.1581, 0.1379, 0.1215, 0.1123, 0.1060, 0.1033, 0.1032, 0.1020]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(ratios,uniform,label='uniform with \\lambda = 10^-4, b = 32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67392a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
