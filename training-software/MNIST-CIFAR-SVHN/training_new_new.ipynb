{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167682b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Keras Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "#Utility\n",
    "# from utils import binarize\n",
    "sys.path.insert(0, '..')\n",
    "from binarization_utils import *\n",
    "# from model_architectures import get_model\n",
    "\n",
    "#Probability\n",
    "import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51147cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c08114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='MNIST'\n",
    "# Train=True\n",
    "# Evaluate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e63703",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6n/905d1knd5gl5czcs5q6mx57c0000gn/T/ipykernel_95185/55694550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#Train Val Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# convert class vectors to binary class matrices\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "#Restructure Datea\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "# Y_train = to_categorical(y_train, 10)\n",
    "# Y_test = to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "\n",
    "#Train Val Split\n",
    "X_train,X_val,y_train, y_val = train_test_split(X_train,y_train,test_size = 0.10)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(y_train.shape[0], 'train samples')\n",
    "\n",
    "print(X_val.shape[0], 'val samples')\n",
    "print(y_val.shape[0], 'val samples')\n",
    "\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(y_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1068183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def computeP(n,k, mu_L, mu_H, var_L, var_H, threshold):\n",
    "#     \"\"\"\n",
    "#     Computes probability of error for a specific input with final inner product k\n",
    "    \n",
    "#     n : Length of vector\n",
    "#     k : input\n",
    "#     mu : Mean\n",
    "#     var : Variance\n",
    "    \n",
    "#     _h: High state\n",
    "#     _l: low state\n",
    "#     \"\"\"\n",
    "\n",
    "#     p01 = []\n",
    "#     p10 = []\n",
    "#     means = [m * mu_L + (n_dists - 1 - m) * mu_H for m in range(0, n_dists)]\n",
    "#     variances = [m**2 * var_L + (n_dists - 1 - m)**2 * var_H for m in range(0, n_dists)]\n",
    "#     for t in range(0,n_dists):\n",
    "#         if t < n_dists - 1 - t:\n",
    "#             p01 = p01 + [1 - norm.cdf(threshold, means[t], np.sqrt(variances[t]))]\n",
    "#         if t > n_dists - 1 - t:\n",
    "#             p10 = p10 + [norm.cdf(threshold, means[t], np.sqrt(variances[t]))]\n",
    "#     return p01, p10\n",
    "\n",
    "\n",
    "class Sign_layer(Layer):\n",
    "    def __init__(self, levels=1,**kwargs):\n",
    "        self.levels=levels\n",
    "        super(Sign_layer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        ars=np.arange(self.levels)+1.0\n",
    "        ars=ars[::-1]\n",
    "        means=ars/np.sum(ars)\n",
    "        self.means=[K.variable(m) for m in means]\n",
    "        self._trainable_weights = self.means\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        resid = x\n",
    "        out_bin=0\n",
    "        for l in range(self.levels):\n",
    "            out=binarize(resid)*(K.abs(self.means[l]))\n",
    "#             print(out)\n",
    "            out_bin=out_bin+out\n",
    "            resid=resid-out\n",
    "        return out_bin\n",
    "    \n",
    "        # the following lines were an idea to implement flips using tensor operations\n",
    "        '''positive_mask = tf.cast(out_bin > 0, tf.float32)\n",
    "        negative_mask = tf.cast(out_bin < 0, tf.float32)\n",
    "        \n",
    "        positive_flips = tf.random.uniform(out_bin.shape) < p[1]\n",
    "        positives = tf.math.multiply(positive_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[1], tf.float32) - 1))\n",
    "        negatives = tf.math.multiply(negative_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[0], tf.float32) - 1))\n",
    "        return'''\n",
    "\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return input_shape\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    def set_means(self,X):\n",
    "        means=np.zeros((self.levels))\n",
    "        means[0]=1\n",
    "        resid=np.clip(X,-1,1)\n",
    "        approx=0\n",
    "        for l in range(self.levels):\n",
    "            m=np.mean(np.absolute(resid))\n",
    "            out=np.sign(resid)*m\n",
    "            approx=approx+out\n",
    "            resid=resid-out\n",
    "            means[l]=m\n",
    "            err=np.mean((approx-np.clip(X,-1,1))**2)\n",
    "\n",
    "        means=means/np.sum(means)\n",
    "        sess=K.get_session()\n",
    "        sess.run(self.means.assign(means))\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'levels': self.levels\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class binary_dense_error_var(Layer):\n",
    "    def __init__(self,n_in,n_out,error_type,error_dict,**kwargs):\n",
    "        self.n_in=n_in\n",
    "        self.n_out=n_out\n",
    "        self.error_type=error_type\n",
    "        self.error_dict= error_dict\n",
    "        assert self.error_type in [\"NoError\", \"Flip\",\"InnerDot\"], \"error_type of {0} is invalid\".format(error_type)\n",
    "        super(binary_dense_error_var,self).__init__(**kwargs)\n",
    "        \n",
    "        if(error_type == \"InnerDot\"):\n",
    "            max_pop = self.n_in\n",
    "            self.uh = self.error_dict[\"uh\"]\n",
    "            self.ul = self.error_dict[\"ul\"]\n",
    "            self.sigl = self.error_dict[\"sigl\"]\n",
    "            self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "            #Not Used, Mainly used for information\n",
    "            means = [m * self.ul + (max_pop - m) * self.uh for m in range(0, max_pop+1)]\n",
    "            stdvs = [np.sqrt(m * (self.sigl**2) + (max_pop - m)*(self.sigh**2)) for m in range(0, max_pop+1)]\n",
    "            self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "    \n",
    "        elif(error_type == \"Flip\"):\n",
    "            self.p = self.error_dict[\"p\"]\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        stdv=1/np.sqrt(self.n_in)\n",
    "        w = np.random.normal(loc=0.0, scale=stdv,size=[self.n_in,self.n_out]).astype(np.float32)\n",
    "        self.w=K.variable(w)\n",
    "        self.gamma_w=K.variable(1.0)\n",
    "        self._trainable_weights=[self.w,self.gamma_w]\n",
    "        \n",
    "        \n",
    "#         if(error_type == \"InnerDot\"):\n",
    "#             max_pop = self.n_in+1\n",
    "#             self.uh = self.error_dict[\"uh\"]\n",
    "#             self.ul = self.error_dict[\"ul\"]\n",
    "#             self.sigl = self.error_dict[\"sigl\"]\n",
    "#             self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             means = [m * ul + (max_pop - 1 - m) * uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (sigl**2) + (max_pop - 1 - m)*(sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "            \n",
    "    def call(self, x,training=None):\n",
    "        \n",
    "        #Designed with Batch norm in mind, Binarization done after. \n",
    "        if(self.error_type == \"NoError\"):\n",
    "            self.clamped_w=binarize(self.w)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"Flip\"):\n",
    "            self.clamped_w=binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"InnerDot\"):\n",
    "            \n",
    "            if training:\n",
    "                \n",
    "                #Perfrom error model on inference\n",
    "                #No Error Model first\n",
    "                self.clamped_w=binarize(self.w)\n",
    "                \n",
    "                self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "                \n",
    "                # Make Model\n",
    "                self.clamped_x=binarize(x)\n",
    "                gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "                #self.prod = K.dot(x,self.clamped_w)\n",
    "                self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "                #Add Error \n",
    "                self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                     scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "                \n",
    "                self.samps = self.dist.sample(1)\n",
    "                self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "        \n",
    "                #Use Histogram for binning\n",
    "                self.step = self.ul - self.uh\n",
    "                self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "                self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "                self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "                \n",
    "                #Gradient not using binning\n",
    "                return  self.prod + K.stop_gradient(self.out -  self.prod)\n",
    "\n",
    "            else:\n",
    "                #Perfrom error model on inference\n",
    "                #No Error Model first\n",
    "                self.clamped_w=binarize(self.w)\n",
    "                self.clamped_x=binarize(x)\n",
    "                gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "                #self.prod = K.dot(x,self.clamped_w)\n",
    "                self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "                #Add Error \n",
    "                self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                     scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "                \n",
    "\n",
    "                self.samps = self.dist.sample(1)\n",
    "                self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "        \n",
    "                #Use Histogram for binning\n",
    "                self.step = self.ul - self.uh\n",
    "                self.edges = np.arange(self.n_in*self.uh-self.step/2, self.n_in*self.ul+self.step, self.step,dtype=np.float32)\n",
    "                self.bins= tfp.stats.find_bins(self.samps, self.edges, extend_lower_interval = True, extend_upper_interval=True)\n",
    "                self.out = K.abs(self.gamma_w)*gamma_x*(2*self.bins-self.n_in)\n",
    "                \n",
    "            \n",
    "                return self.out\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_in': self.n_in,\n",
    "            'n_out': self.n_out,\n",
    "            'error_type': self.error_type,\n",
    "            'error_dict': self.error_dict\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ca1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing the Code\n",
    "# #Model Parameters\n",
    "# # 0 : No error, params ignored\n",
    "# # 1 : Flip error, only p \n",
    "\n",
    "# # 2 : InnerProd error\n",
    "# # error_type = \"NoError\"\n",
    "# # error_dict = {}\n",
    "\n",
    "# #Flip Probabiliy\n",
    "# p=0.1\n",
    "# # error_type = \"Flip\"\n",
    "# # error_dict = {\"p\": p }\n",
    "\n",
    "# #InnerProd paramslayer.call([0,0,0])\n",
    "# ul = 1\n",
    "# uh = 0.01\n",
    "# sigl = 0.01\n",
    "# sigh = 0.01\n",
    "\n",
    "# error_type = \"InnerDot\"\n",
    "# error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "\n",
    "# layer = binary_dense_error_var(n_in=3,n_out=1,error_type = error_type, error_dict = error_dict)\n",
    "\n",
    "# out = layer.__call__(tf.constant([[1,1,-1], [1,1,1],[-1,-1,-1]], dtype = tf.float32), training=False)\n",
    "# print(layer.clamped_w.numpy())\n",
    "# print(layer.pop.numpy())\n",
    "# print(layer.samps.numpy())\n",
    "# print(layer.edges)\n",
    "# print(layer.bins.numpy())\n",
    "# print(out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8db1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Construction\n",
    "\n",
    "def make_MNIST_model(error_type, p = 0.1, ul = 1, sigl = 0.01, uh=0.01, sigh = 0.01 ):\n",
    "    if(error_type == 0):\n",
    "        #No Error\n",
    "        error_type = \"NoError\"\n",
    "        error_dict = {}\n",
    "    elif(error_type == 1):\n",
    "        #Flip Error\n",
    "        error_type = \"Flip\"\n",
    "        error_dict = {\"p\": p }\n",
    "    elif(error_type == 2):\n",
    "        #InnerDot error \n",
    "        error_type = \"InnerDot\"\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "        \n",
    "    #Fixed Parameters \n",
    "    batch_norm_eps=1e-4\n",
    "    batch_norm_alpha=0.1#(this is same as momentum)\n",
    "    \n",
    "    #Model Def\n",
    "    #5 Binary dense layers,final layer is softmax\n",
    "    model=Sequential()\n",
    "    #First Layer Group\n",
    "    model.add(binary_dense_error_var(\n",
    "        n_in=784,n_out=256,input_shape=[784],error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=1))\n",
    "\n",
    "    #Subsequent Dense Layers\n",
    "    #Change Layer Sizes\n",
    "#     layer_sizes = [1024,1024]\n",
    "    layer_sizes = [256,256,256]\n",
    "    for i in range(len(layer_sizes)):\n",
    "        model.add(binary_dense_error_var(\n",
    "            n_in=int(model.output.get_shape()[1]),n_out=layer_sizes[i],error_type = error_type, error_dict = error_dict))\n",
    "        model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "        model.add(Sign_layer(levels=1))\n",
    "\n",
    "    \n",
    "    #Final Layer\n",
    "    model.add(binary_dense_error_var(\n",
    "            n_in=int(model.output.get_shape()[1]),n_out=10,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the following is a workaround so that the model weights can be saved\n",
    "    # https://github.com/tensorflow/tensorflow/issues/46871\n",
    "    j = 0\n",
    "    for w in model.weights:\n",
    "        w._handle_name = 'model_' + str(j) + w.name\n",
    "        j = j + 1\n",
    "        \n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_CIFAR10_model(error_type, p = 0.1, ul = 1, sigl = 0.01, uh=0.01, sigh = 0.01 ):\n",
    "    if(error_type == 0):\n",
    "        #No Error\n",
    "        error_type = \"NoError\"\n",
    "        error_dict = {}\n",
    "    elif(error_type == 1):\n",
    "        #Flip Error\n",
    "        error_type = \"Flip\"\n",
    "        error_dict = {\"p\": p }\n",
    "    elif(error_type == 2):\n",
    "        #InnerDot error \n",
    "        error_type = \"InnerDot\"\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "        \n",
    "    #Fixed Parameters \n",
    "    batch_norm_eps=1e-4\n",
    "    batch_norm_alpha=0.1#(this is same as momentum)\n",
    "    \n",
    "    #Model Def\n",
    "    model=Sequential()\n",
    "    model.add(binary_conv(nfilters=64,ch_in=3,k=3,padding='valid',input_shape=[32,32,3]))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(binary_conv(nfilters=64,ch_in=64,k=3,padding='valid'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(binary_conv(nfilters=128,ch_in=64,k=3,padding='valid'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(binary_conv(nfilters=128,ch_in=128,k=3,padding='valid'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(binary_conv(nfilters=256,ch_in=128,k=3,padding='valid'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(binary_conv(nfilters=256,ch_in=256,k=3,padding='valid'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(my_flat())\n",
    "\n",
    "    model.add(binary_dense_error_var(n_in=int(model.output.get_shape()[1]),n_out=512,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(binary_dense_error_var(n_in=int(model.output.get_shape()[1]),n_out=512,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=resid_levels))\n",
    "    model.add(binary_dense_error_var(n_in=int(model.output.get_shape()[1]),n_out=10,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the following is a workaround so that the model weights can be saved\n",
    "    # https://github.com/tensorflow/tensorflow/issues/46871\n",
    "    j = 0\n",
    "    for w in model.weights:\n",
    "        w._handle_name = 'model_' + str(j) + w.name\n",
    "        j = j + 1\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8117e",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1879470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_280\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "binary_dense_error_var_1385  (None, 256)               200705    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1385 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_1108 (Sign_layer) (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_1386  (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1386 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_1109 (Sign_layer) (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_1387  (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1387 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_1110 (Sign_layer) (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_1388  (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1388 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_1111 (Sign_layer) (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_1389  (None, 10)                2561      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1389 (Ba (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 404,017\n",
      "Trainable params: 401,949\n",
      "Non-trainable params: 2,068\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "error_type = 2\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "\n",
    "#InnerProd params\n",
    "ul = 1 #l-low means bits match\n",
    "uh = 0.1\n",
    "sigl = 0.01\n",
    "sigh = 0.2\n",
    "\n",
    "#Make Model\n",
    "model = make_MNIST_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e13ab77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1080/1080 - 13s - loss: 0.4406 - accuracy: 0.8786 - val_loss: 0.2317 - val_accuracy: 0.9335\n",
      "Epoch 2/100\n",
      "1080/1080 - 12s - loss: 0.2890 - accuracy: 0.9182 - val_loss: 0.2408 - val_accuracy: 0.9323\n",
      "Epoch 3/100\n",
      "1080/1080 - 11s - loss: 0.2406 - accuracy: 0.9315 - val_loss: 0.2098 - val_accuracy: 0.9400\n",
      "Epoch 4/100\n",
      "1080/1080 - 13s - loss: 0.2126 - accuracy: 0.9387 - val_loss: 0.1843 - val_accuracy: 0.9487\n",
      "Epoch 5/100\n",
      "1080/1080 - 11s - loss: 0.1911 - accuracy: 0.9452 - val_loss: 0.1704 - val_accuracy: 0.9517\n",
      "Epoch 6/100\n",
      "1080/1080 - 12s - loss: 0.1786 - accuracy: 0.9483 - val_loss: 0.1728 - val_accuracy: 0.9495\n",
      "Epoch 7/100\n",
      "1080/1080 - 12s - loss: 0.1662 - accuracy: 0.9513 - val_loss: 0.1718 - val_accuracy: 0.9525\n",
      "Epoch 8/100\n",
      "1080/1080 - 11s - loss: 0.1546 - accuracy: 0.9551 - val_loss: 0.1967 - val_accuracy: 0.9415\n",
      "Epoch 9/100\n",
      "1080/1080 - 11s - loss: 0.1502 - accuracy: 0.9564 - val_loss: 0.1943 - val_accuracy: 0.9458\n",
      "Epoch 10/100\n",
      "1080/1080 - 12s - loss: 0.1431 - accuracy: 0.9579 - val_loss: 0.1966 - val_accuracy: 0.9520\n",
      "Epoch 11/100\n",
      "1080/1080 - 12s - loss: 0.1334 - accuracy: 0.9600 - val_loss: 0.1704 - val_accuracy: 0.9523\n",
      "Epoch 12/100\n",
      "1080/1080 - 12s - loss: 0.1315 - accuracy: 0.9615 - val_loss: 0.2107 - val_accuracy: 0.9425\n",
      "Epoch 13/100\n",
      "1080/1080 - 12s - loss: 0.1283 - accuracy: 0.9618 - val_loss: 0.1677 - val_accuracy: 0.9533\n",
      "Epoch 14/100\n",
      "1080/1080 - 13s - loss: 0.1180 - accuracy: 0.9651 - val_loss: 0.1934 - val_accuracy: 0.9505\n",
      "Epoch 15/100\n",
      "1080/1080 - 15s - loss: 0.1196 - accuracy: 0.9641 - val_loss: 0.1810 - val_accuracy: 0.9483\n",
      "Epoch 16/100\n",
      "1080/1080 - 16s - loss: 0.1180 - accuracy: 0.9650 - val_loss: 0.1572 - val_accuracy: 0.9555\n",
      "Epoch 17/100\n",
      "1080/1080 - 15s - loss: 0.1125 - accuracy: 0.9668 - val_loss: 0.1421 - val_accuracy: 0.9603\n",
      "Epoch 18/100\n",
      "1080/1080 - 15s - loss: 0.1094 - accuracy: 0.9666 - val_loss: 0.1516 - val_accuracy: 0.9592\n",
      "Epoch 19/100\n",
      "1080/1080 - 15s - loss: 0.1034 - accuracy: 0.9682 - val_loss: 0.1493 - val_accuracy: 0.9577\n",
      "Epoch 20/100\n",
      "1080/1080 - 14s - loss: 0.1070 - accuracy: 0.9686 - val_loss: 0.1420 - val_accuracy: 0.9573\n",
      "Epoch 21/100\n",
      "1080/1080 - 16s - loss: 0.1035 - accuracy: 0.9679 - val_loss: 0.1856 - val_accuracy: 0.9482\n",
      "Epoch 22/100\n",
      "1080/1080 - 15s - loss: 0.1007 - accuracy: 0.9693 - val_loss: 0.1689 - val_accuracy: 0.9520\n",
      "Epoch 23/100\n",
      "1080/1080 - 14s - loss: 0.0972 - accuracy: 0.9706 - val_loss: 0.1350 - val_accuracy: 0.9633\n",
      "Epoch 24/100\n",
      "1080/1080 - 14s - loss: 0.0973 - accuracy: 0.9708 - val_loss: 0.1556 - val_accuracy: 0.9585\n",
      "Epoch 25/100\n",
      "1080/1080 - 12s - loss: 0.0935 - accuracy: 0.9719 - val_loss: 0.1948 - val_accuracy: 0.9458\n",
      "Epoch 26/100\n",
      "1080/1080 - 14s - loss: 0.0942 - accuracy: 0.9710 - val_loss: 0.1422 - val_accuracy: 0.9622\n",
      "Epoch 27/100\n",
      "1080/1080 - 16s - loss: 0.0916 - accuracy: 0.9724 - val_loss: 0.1529 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "1080/1080 - 14s - loss: 0.0894 - accuracy: 0.9722 - val_loss: 0.1318 - val_accuracy: 0.9662\n",
      "Epoch 29/100\n",
      "1080/1080 - 14s - loss: 0.0906 - accuracy: 0.9721 - val_loss: 0.1374 - val_accuracy: 0.9638\n",
      "Epoch 30/100\n",
      "1080/1080 - 14s - loss: 0.0872 - accuracy: 0.9733 - val_loss: 0.1330 - val_accuracy: 0.9600\n",
      "Epoch 31/100\n",
      "1080/1080 - 15s - loss: 0.0879 - accuracy: 0.9731 - val_loss: 0.1713 - val_accuracy: 0.9543\n",
      "Epoch 32/100\n",
      "1080/1080 - 15s - loss: 0.0846 - accuracy: 0.9737 - val_loss: 0.1565 - val_accuracy: 0.9578\n",
      "Epoch 33/100\n",
      "1080/1080 - 13s - loss: 0.0859 - accuracy: 0.9738 - val_loss: 0.1701 - val_accuracy: 0.9530\n",
      "Epoch 34/100\n",
      "1080/1080 - 13s - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.1218 - val_accuracy: 0.9677\n",
      "Epoch 35/100\n",
      "1080/1080 - 14s - loss: 0.0818 - accuracy: 0.9753 - val_loss: 0.1458 - val_accuracy: 0.9598\n",
      "Epoch 36/100\n",
      "1080/1080 - 15s - loss: 0.0805 - accuracy: 0.9752 - val_loss: 0.1335 - val_accuracy: 0.9627\n",
      "Epoch 37/100\n",
      "1080/1080 - 15s - loss: 0.0819 - accuracy: 0.9751 - val_loss: 0.1619 - val_accuracy: 0.9567\n",
      "Epoch 38/100\n",
      "1080/1080 - 15s - loss: 0.0814 - accuracy: 0.9752 - val_loss: 0.1292 - val_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "1080/1080 - 15s - loss: 0.0822 - accuracy: 0.9745 - val_loss: 0.1418 - val_accuracy: 0.9612\n",
      "Epoch 40/100\n",
      "1080/1080 - 15s - loss: 0.0785 - accuracy: 0.9760 - val_loss: 0.1533 - val_accuracy: 0.9587\n",
      "Epoch 41/100\n",
      "1080/1080 - 15s - loss: 0.0791 - accuracy: 0.9760 - val_loss: 0.1738 - val_accuracy: 0.9557\n",
      "Epoch 42/100\n",
      "1080/1080 - 16s - loss: 0.0802 - accuracy: 0.9758 - val_loss: 0.1407 - val_accuracy: 0.9618\n",
      "Epoch 43/100\n",
      "1080/1080 - 14s - loss: 0.0792 - accuracy: 0.9750 - val_loss: 0.1510 - val_accuracy: 0.9603\n",
      "Epoch 44/100\n",
      "1080/1080 - 14s - loss: 0.0799 - accuracy: 0.9747 - val_loss: 0.1319 - val_accuracy: 0.9660\n",
      "Epoch 45/100\n",
      "1080/1080 - 15s - loss: 0.0761 - accuracy: 0.9768 - val_loss: 0.1828 - val_accuracy: 0.9540\n",
      "Epoch 46/100\n",
      "1080/1080 - 16s - loss: 0.0786 - accuracy: 0.9760 - val_loss: 0.1374 - val_accuracy: 0.9640\n",
      "Epoch 47/100\n",
      "1080/1080 - 15s - loss: 0.0756 - accuracy: 0.9764 - val_loss: 0.1802 - val_accuracy: 0.9585\n",
      "Epoch 48/100\n",
      "1080/1080 - 17s - loss: 0.0777 - accuracy: 0.9758 - val_loss: 0.1430 - val_accuracy: 0.9613\n",
      "Epoch 49/100\n",
      "1080/1080 - 15s - loss: 0.0779 - accuracy: 0.9756 - val_loss: 0.1393 - val_accuracy: 0.9613\n",
      "Epoch 50/100\n",
      "1080/1080 - 16s - loss: 0.0731 - accuracy: 0.9775 - val_loss: 0.1550 - val_accuracy: 0.9580\n",
      "Epoch 51/100\n",
      "1080/1080 - 15s - loss: 0.0756 - accuracy: 0.9765 - val_loss: 0.2031 - val_accuracy: 0.9545\n",
      "Epoch 52/100\n",
      "1080/1080 - 14s - loss: 0.0766 - accuracy: 0.9768 - val_loss: 0.1517 - val_accuracy: 0.9580\n",
      "Epoch 53/100\n",
      "1080/1080 - 13s - loss: 0.0724 - accuracy: 0.9770 - val_loss: 0.1466 - val_accuracy: 0.9598\n",
      "Epoch 54/100\n",
      "1080/1080 - 13s - loss: 0.0740 - accuracy: 0.9769 - val_loss: 0.1514 - val_accuracy: 0.9610\n",
      "Epoch 55/100\n",
      "1080/1080 - 12s - loss: 0.0754 - accuracy: 0.9763 - val_loss: 0.1600 - val_accuracy: 0.9607\n",
      "Epoch 56/100\n",
      "1080/1080 - 13s - loss: 0.0757 - accuracy: 0.9767 - val_loss: 0.1616 - val_accuracy: 0.9558\n",
      "Epoch 57/100\n",
      "1080/1080 - 12s - loss: 0.0754 - accuracy: 0.9767 - val_loss: 0.1266 - val_accuracy: 0.9623\n",
      "Epoch 58/100\n",
      "1080/1080 - 14s - loss: 0.0760 - accuracy: 0.9765 - val_loss: 0.1503 - val_accuracy: 0.9613\n",
      "Epoch 59/100\n",
      "1080/1080 - 15s - loss: 0.0730 - accuracy: 0.9776 - val_loss: 0.1509 - val_accuracy: 0.9585\n",
      "Epoch 60/100\n",
      "1080/1080 - 15s - loss: 0.0715 - accuracy: 0.9774 - val_loss: 0.1465 - val_accuracy: 0.9598\n",
      "Epoch 61/100\n",
      "1080/1080 - 15s - loss: 0.0731 - accuracy: 0.9782 - val_loss: 0.1402 - val_accuracy: 0.9635\n",
      "Epoch 62/100\n",
      "1080/1080 - 15s - loss: 0.0723 - accuracy: 0.9768 - val_loss: 0.1639 - val_accuracy: 0.9557\n",
      "Epoch 63/100\n",
      "1080/1080 - 15s - loss: 0.0745 - accuracy: 0.9762 - val_loss: 0.1403 - val_accuracy: 0.9650\n",
      "Epoch 64/100\n",
      "1080/1080 - 15s - loss: 0.0727 - accuracy: 0.9776 - val_loss: 0.1275 - val_accuracy: 0.9638\n",
      "Epoch 65/100\n",
      "1080/1080 - 17s - loss: 0.0751 - accuracy: 0.9767 - val_loss: 0.1316 - val_accuracy: 0.9650\n",
      "Epoch 66/100\n",
      "1080/1080 - 17s - loss: 0.0724 - accuracy: 0.9771 - val_loss: 0.1543 - val_accuracy: 0.9605\n",
      "Epoch 67/100\n",
      "1080/1080 - 14s - loss: 0.0716 - accuracy: 0.9775 - val_loss: 0.1586 - val_accuracy: 0.9582\n",
      "Epoch 68/100\n",
      "1080/1080 - 13s - loss: 0.0750 - accuracy: 0.9774 - val_loss: 0.1565 - val_accuracy: 0.9608\n",
      "Epoch 69/100\n",
      "1080/1080 - 15s - loss: 0.0726 - accuracy: 0.9778 - val_loss: 0.1567 - val_accuracy: 0.9578\n",
      "Epoch 70/100\n",
      "1080/1080 - 13s - loss: 0.0730 - accuracy: 0.9766 - val_loss: 0.1361 - val_accuracy: 0.9648\n",
      "Epoch 71/100\n",
      "1080/1080 - 12s - loss: 0.0720 - accuracy: 0.9779 - val_loss: 0.1666 - val_accuracy: 0.9608\n",
      "Epoch 72/100\n",
      "1080/1080 - 15s - loss: 0.0713 - accuracy: 0.9781 - val_loss: 0.1260 - val_accuracy: 0.9662\n",
      "Epoch 73/100\n",
      "1080/1080 - 14s - loss: 0.0737 - accuracy: 0.9779 - val_loss: 0.1263 - val_accuracy: 0.9640\n",
      "Epoch 74/100\n",
      "1080/1080 - 14s - loss: 0.0749 - accuracy: 0.9765 - val_loss: 0.1245 - val_accuracy: 0.9662\n",
      "Epoch 75/100\n",
      "1080/1080 - 14s - loss: 0.0693 - accuracy: 0.9786 - val_loss: 0.1477 - val_accuracy: 0.9607\n",
      "Epoch 76/100\n",
      "1080/1080 - 14s - loss: 0.0707 - accuracy: 0.9784 - val_loss: 0.1784 - val_accuracy: 0.9555\n",
      "Epoch 77/100\n",
      "1080/1080 - 14s - loss: 0.0709 - accuracy: 0.9782 - val_loss: 0.1505 - val_accuracy: 0.9613\n",
      "Epoch 78/100\n",
      "1080/1080 - 14s - loss: 0.0690 - accuracy: 0.9791 - val_loss: 0.1498 - val_accuracy: 0.9617\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080/1080 - 14s - loss: 0.0693 - accuracy: 0.9786 - val_loss: 0.1437 - val_accuracy: 0.9647\n",
      "Epoch 80/100\n",
      "1080/1080 - 13s - loss: 0.0750 - accuracy: 0.9767 - val_loss: 0.1441 - val_accuracy: 0.9625\n",
      "Epoch 81/100\n",
      "1080/1080 - 14s - loss: 0.0727 - accuracy: 0.9772 - val_loss: 0.1270 - val_accuracy: 0.9665\n",
      "Epoch 82/100\n",
      "1080/1080 - 14s - loss: 0.0708 - accuracy: 0.9785 - val_loss: 0.1255 - val_accuracy: 0.9662\n",
      "Epoch 83/100\n",
      "1080/1080 - 13s - loss: 0.0721 - accuracy: 0.9781 - val_loss: 0.1217 - val_accuracy: 0.9662\n",
      "Epoch 84/100\n",
      "1080/1080 - 15s - loss: 0.0711 - accuracy: 0.9778 - val_loss: 0.1384 - val_accuracy: 0.9660\n",
      "Epoch 85/100\n",
      "1080/1080 - 12s - loss: 0.0729 - accuracy: 0.9779 - val_loss: 0.1314 - val_accuracy: 0.9630\n",
      "Epoch 86/100\n",
      "1080/1080 - 12s - loss: 0.0713 - accuracy: 0.9776 - val_loss: 0.1423 - val_accuracy: 0.9623\n",
      "Epoch 87/100\n",
      "1080/1080 - 13s - loss: 0.0718 - accuracy: 0.9779 - val_loss: 0.1556 - val_accuracy: 0.9568\n",
      "Epoch 88/100\n",
      "1080/1080 - 11s - loss: 0.0712 - accuracy: 0.9777 - val_loss: 0.1484 - val_accuracy: 0.9602\n",
      "Epoch 89/100\n",
      "1080/1080 - 13s - loss: 0.0682 - accuracy: 0.9786 - val_loss: 0.1505 - val_accuracy: 0.9608\n",
      "Epoch 90/100\n",
      "1080/1080 - 13s - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.2119 - val_accuracy: 0.9512\n",
      "Epoch 91/100\n",
      "1080/1080 - 14s - loss: 0.0677 - accuracy: 0.9796 - val_loss: 0.1397 - val_accuracy: 0.9623\n",
      "Epoch 92/100\n",
      "1080/1080 - 16s - loss: 0.0693 - accuracy: 0.9790 - val_loss: 0.1410 - val_accuracy: 0.9655\n",
      "Epoch 93/100\n",
      "1080/1080 - 14s - loss: 0.0715 - accuracy: 0.9777 - val_loss: 0.1611 - val_accuracy: 0.9608\n",
      "Epoch 94/100\n",
      "1080/1080 - 14s - loss: 0.0709 - accuracy: 0.9784 - val_loss: 0.1432 - val_accuracy: 0.9603\n",
      "Epoch 95/100\n",
      "1080/1080 - 14s - loss: 0.0710 - accuracy: 0.9783 - val_loss: 0.1455 - val_accuracy: 0.9615\n",
      "Epoch 96/100\n",
      "1080/1080 - 14s - loss: 0.0707 - accuracy: 0.9784 - val_loss: 0.1345 - val_accuracy: 0.9640\n",
      "Epoch 97/100\n",
      "1080/1080 - 14s - loss: 0.0717 - accuracy: 0.9782 - val_loss: 0.1385 - val_accuracy: 0.9642\n",
      "Epoch 98/100\n",
      "1080/1080 - 15s - loss: 0.0689 - accuracy: 0.9785 - val_loss: 0.1644 - val_accuracy: 0.9557\n",
      "Epoch 99/100\n",
      "1080/1080 - 13s - loss: 0.0694 - accuracy: 0.9784 - val_loss: 0.1323 - val_accuracy: 0.9643\n",
      "Epoch 100/100\n",
      "1080/1080 - 13s - loss: 0.0690 - accuracy: 0.9784 - val_loss: 0.1333 - val_accuracy: 0.9603\n"
     ]
    }
   ],
   "source": [
    "#Training Script\n",
    "    \n",
    "#Training parameters\n",
    "batch_size=50\n",
    "epochs=100\n",
    "\n",
    "#Makes Model Name\n",
    "if(error_type == 0):\n",
    "    #No Error\n",
    "    model_name = \"error_{0}\".format(error_type)\n",
    "elif(error_type == 1):\n",
    "    model_name = \"error_{0}_p_{1}\".format(error_type,p)\n",
    "elif(error_type == 2):\n",
    "    #InnerDot error \n",
    "    model_name = \"error_{0}_ul_{1}_uh_{2}_sigl_{3}_sigh_{4}\".format(error_type,ul,uh,sigl,sigh)\n",
    "\n",
    "if not(os.path.exists('models')):\n",
    "    os.mkdir('models')\n",
    "if not(os.path.exists('models/'+model_name)):\n",
    "    os.mkdir('models/'+model_name)\n",
    "    \n",
    "    \n",
    "# #gather all binary dense and binary convolution layers:\n",
    "# binary_layers=[]\n",
    "# for l in model.layers:\n",
    "#     if isinstance(l,binary_dense) or isinstance(l,binary_conv):\n",
    "#         binary_layers.append(l)\n",
    "\n",
    "#Train\n",
    "lr=0.01\n",
    "opt = keras.optimizers.Adam(learning_rate=lr,decay=1e-6)#SGD(lr=lr,momentum=0.9,decay=1e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "\n",
    "\n",
    "weights_path='models/'+model_name+'/'+model_name+'.h5'\n",
    "cback=keras.callbacks.ModelCheckpoint(weights_path, monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n",
    "\n",
    "\n",
    "#Training\n",
    "if keras.__version__[0]=='2':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,epochs=epochs,callbacks=[cback])\n",
    "if keras.__version__[0]=='1':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,nb_epoch=epochs,callbacks=[cback])\n",
    "\n",
    "#Save history    \n",
    "dic={'hard':history.history}\n",
    "foo=open('models/'+model_name+'.pkl','wb')\n",
    "pickle.dump(dic,foo)\n",
    "foo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb271c57",
   "metadata": {},
   "source": [
    "# Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80b30a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "5.157894736842105\n",
      "9.31578947368421\n",
      "13.473684210526315\n",
      "17.63157894736842\n",
      "21.789473684210527\n",
      "25.94736842105263\n",
      "30.105263157894736\n",
      "34.26315789473684\n",
      "38.421052631578945\n",
      "42.578947368421055\n",
      "46.73684210526316\n",
      "50.89473684210526\n",
      "55.05263157894737\n",
      "59.21052631578947\n",
      "63.36842105263158\n",
      "67.52631578947368\n",
      "71.6842105263158\n",
      "75.84210526315789\n",
      "80.0\n",
      "0.9714\n",
      "0.9699\n",
      "0.9691\n",
      "0.9685\n",
      "0.9633\n",
      "0.9575\n",
      "0.9544\n",
      "0.9463\n",
      "0.9379\n",
      "0.9218\n",
      "0.9078\n",
      "0.8877\n",
      "0.8589\n",
      "0.8193\n",
      "0.7874\n",
      "0.7466\n",
      "0.7129\n",
      "0.6569\n",
      "0.6226\n",
      "0.5587\n"
     ]
    }
   ],
   "source": [
    "#Test Based on Error Type\n",
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "train_error_type = 2 #can be 0 or 1 or 2\n",
    "test_error_type = 2 # can be 0 1 or 2\n",
    "train_flip_probability = 0.1\n",
    "\n",
    "#Training InnerProd params\n",
    "train_ul = 1\n",
    "train_uh = 0.1\n",
    "train_sigl = 0.01\n",
    "train_sigh = 0.1\n",
    "\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "\n",
    "# Test Parameters  \n",
    "#InnerProd params\n",
    "ul = 1\n",
    "uh = 0.1\n",
    "sigl = 0.01\n",
    "sigh = 0.01\n",
    "\n",
    "#Makes Model Name\n",
    "if(train_error_type == 0):\n",
    "    #No Error\n",
    "    model_name = \"error_{0}\".format(train_error_type)\n",
    "elif(train_error_type == 1):\n",
    "    model_name = \"error_{0}_p_{1}\".format(train_error_type,train_flip_probability)\n",
    "elif(train_error_type == 2):\n",
    "    #InnerDot error \n",
    "    model_name = \"error_{0}_ul_{1}_uh_{2}_sigl_{3}_sigh_{4}\".format(train_error_type,train_ul,train_uh,train_sigl,train_sigh)    \n",
    "    \n",
    "weights_path = 'models/'+model_name+'/'+model_name+'.h5' #To Specify  \n",
    "\n",
    "#weights_path = \"\" #To Specify\n",
    "\n",
    "if(test_error_type == 0):\n",
    "    #No Error\n",
    "    model=make_MNIST_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "    model.compile()\n",
    "\n",
    "    #Load Weights\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    score=model.evaluate(X_test,y_test,verbose=0)\n",
    "    print(\"No Error, test loss was %0.4f, test accuracy was %0.4f\"%(score[0],score[1]))\n",
    "\n",
    "elif(test_error_type == 1):\n",
    "    #Flip Error\n",
    "    probs = np.logspace(-4, -1, 20)\n",
    "    \n",
    "    acc_1 = []\n",
    "    for p in probs:\n",
    "        \n",
    "        model=make_MNIST_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "        model.compile()\n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        #print(\"With p= %0.4f, test loss was %0.4f, test accuracy was %0.4f\" %(p,score[0],score[1]))\n",
    "        print('%0.4f' %(score[1]))\n",
    "        acc_1.append(score[1])\n",
    "    \n",
    "elif(test_error_type == 2):\n",
    "    #InnerDot Error\n",
    "    error_type = \"InnerDot\"\n",
    "    \n",
    "    ratio_l_h = np.linspace(1,80,20)\n",
    "    acc_2 = []\n",
    "    for ratio in ratio_l_h:\n",
    "        print(ratio)\n",
    "    for ratio in ratio_l_h:\n",
    "        \n",
    "        model = make_MNIST_model(test_error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = ratio*sigl )\n",
    "        \n",
    "        #model.compile()\n",
    "        lr=0.01\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr,decay=1e-6)#SGD(lr=lr,momentum=0.9,decay=1e-5)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy']) \n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        #print(\"With ratio %0.4f, test loss was %0.4f, test accuracy was %0.4f\"%(ratio,score[0],score[1]))\n",
    "        print('%0.4f' %(score[1]))\n",
    "        #print(score)\n",
    "        acc_2.append(score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfeca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
