{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167682b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Keras Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "#Utility\n",
    "# from utils import binarize\n",
    "sys.path.insert(0, '..')\n",
    "from binarization_utils import *\n",
    "# from model_architectures import get_model\n",
    "\n",
    "#Probability\n",
    "import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51147cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c08114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='MNIST'\n",
    "# Train=True\n",
    "# Evaluate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e63703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (54000, 784)\n",
      "54000 train samples\n",
      "54000 train samples\n",
      "6000 val samples\n",
      "6000 val samples\n",
      "10000 test samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# convert class vectors to binary class matrices\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "#Restructure Datea\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "# Y_train = to_categorical(y_train, 10)\n",
    "# Y_test = to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "\n",
    "#Train Val Split\n",
    "X_train,X_val,y_train, y_val = train_test_split(X_train,y_train,test_size = 0.10)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(y_train.shape[0], 'train samples')\n",
    "\n",
    "print(X_val.shape[0], 'val samples')\n",
    "print(y_val.shape[0], 'val samples')\n",
    "\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(y_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1068183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def computeP(n,k, mu_L, mu_H, var_L, var_H, threshold):\n",
    "#     \"\"\"\n",
    "#     Computes probability of error for a specific input with final inner product k\n",
    "    \n",
    "#     n : Length of vector\n",
    "#     k : input\n",
    "#     mu : Mean\n",
    "#     var : Variance\n",
    "    \n",
    "#     _h: High state\n",
    "#     _l: low state\n",
    "#     \"\"\"\n",
    "\n",
    "#     p01 = []\n",
    "#     p10 = []\n",
    "#     means = [m * mu_L + (n_dists - 1 - m) * mu_H for m in range(0, n_dists)]\n",
    "#     variances = [m**2 * var_L + (n_dists - 1 - m)**2 * var_H for m in range(0, n_dists)]\n",
    "#     for t in range(0,n_dists):\n",
    "#         if t < n_dists - 1 - t:\n",
    "#             p01 = p01 + [1 - norm.cdf(threshold, means[t], np.sqrt(variances[t]))]\n",
    "#         if t > n_dists - 1 - t:\n",
    "#             p10 = p10 + [norm.cdf(threshold, means[t], np.sqrt(variances[t]))]\n",
    "#     return p01, p10\n",
    "\n",
    "\n",
    "class Sign_layer(Layer):\n",
    "    def __init__(self, levels=1,**kwargs):\n",
    "        self.levels=levels\n",
    "        super(Sign_layer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        ars=np.arange(self.levels)+1.0\n",
    "        ars=ars[::-1]\n",
    "        means=ars/np.sum(ars)\n",
    "        self.means=[K.variable(m) for m in means]\n",
    "        self._trainable_weights = self.means\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        resid = x\n",
    "        out_bin=0\n",
    "        for l in range(self.levels):\n",
    "            out=binarize(resid)*(K.abs(self.means[l]))\n",
    "#             print(out)\n",
    "            out_bin=out_bin+out\n",
    "            resid=resid-out\n",
    "        return out_bin\n",
    "    \n",
    "        # the following lines were an idea to implement flips using tensor operations\n",
    "        '''positive_mask = tf.cast(out_bin > 0, tf.float32)\n",
    "        negative_mask = tf.cast(out_bin < 0, tf.float32)\n",
    "        \n",
    "        positive_flips = tf.random.uniform(out_bin.shape) < p[1]\n",
    "        positives = tf.math.multiply(positive_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[1], tf.float32) - 1))\n",
    "        negatives = tf.math.multiply(negative_mask, (tf.cast(tf.random.uniform(out_bin.shape) < p[0], tf.float32) - 1))\n",
    "        return'''\n",
    "\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return input_shape\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    def set_means(self,X):\n",
    "        means=np.zeros((self.levels))\n",
    "        means[0]=1\n",
    "        resid=np.clip(X,-1,1)\n",
    "        approx=0\n",
    "        for l in range(self.levels):\n",
    "            m=np.mean(np.absolute(resid))\n",
    "            out=np.sign(resid)*m\n",
    "            approx=approx+out\n",
    "            resid=resid-out\n",
    "            means[l]=m\n",
    "            err=np.mean((approx-np.clip(X,-1,1))**2)\n",
    "\n",
    "        means=means/np.sum(means)\n",
    "        sess=K.get_session()\n",
    "        sess.run(self.means.assign(means))\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'levels': self.levels\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class binary_dense_error_var(Layer):\n",
    "    def __init__(self,n_in,n_out,error_type,error_dict,**kwargs):\n",
    "        self.n_in=n_in\n",
    "        self.n_out=n_out\n",
    "        self.error_type=error_type\n",
    "        self.error_dict= error_dict\n",
    "        assert self.error_type in [\"NoError\", \"Flip\",\"InnerDot\"], \"error_type of {0} is invalid\".format(error_type)\n",
    "        super(binary_dense_error_var,self).__init__(**kwargs)\n",
    "        \n",
    "        if(error_type == \"InnerDot\"):\n",
    "            max_pop = self.n_in\n",
    "            self.uh = self.error_dict[\"uh\"]\n",
    "            self.ul = self.error_dict[\"ul\"]\n",
    "            self.sigl = self.error_dict[\"sigl\"]\n",
    "            self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "            #Not Used, Mainly used for information\n",
    "            means = [m * self.ul + (max_pop - m) * self.uh for m in range(0, max_pop+1)]\n",
    "            stdvs = [np.sqrt(m * (self.sigl**2) + (max_pop - m)*(self.sigh**2)) for m in range(0, max_pop+1)]\n",
    "            self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "    \n",
    "        elif(error_type == \"Flip\"):\n",
    "            self.p = self.error_dict[\"p\"]\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        stdv=1/np.sqrt(self.n_in)\n",
    "        w = np.random.normal(loc=0.0, scale=stdv,size=[self.n_in,self.n_out]).astype(np.float32)\n",
    "        self.w=K.variable(w)\n",
    "        self.gamma_w=K.variable(1.0)\n",
    "        self._trainable_weights=[self.w,self.gamma_w]\n",
    "        \n",
    "        \n",
    "#         if(error_type == \"InnerDot\"):\n",
    "#             max_pop = self.n_in+1\n",
    "#             self.uh = self.error_dict[\"uh\"]\n",
    "#             self.ul = self.error_dict[\"ul\"]\n",
    "#             self.sigl = self.error_dict[\"sigl\"]\n",
    "#             self.sigh = self.error_dict[\"sigh\"]\n",
    "\n",
    "#             means = [m * ul + (max_pop - 1 - m) * uh for m in range(0, max_pop+1)]\n",
    "#             stdvs = [np.sqrt(m * (sigl**2) + (max_pop - 1 - m)*(sigh**2)) for m in range(0, max_pop+1)]\n",
    "#             self.distributions = [tfp.distributions.Normal(loc= m, scale = sig) for m,sig in zip(means,stdvs)]\n",
    "            \n",
    "    def call(self, x,mask=None):\n",
    "        \n",
    "        #Designed with Batch norm in mind, Binarization done after. \n",
    "        if(self.error_type == \"NoError\"):\n",
    "            self.clamped_w=binarize(self.w)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"Flip\"):\n",
    "            self.clamped_w=binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "            self.prod=K.abs(self.gamma_w)*K.dot(x,self.clamped_w)\n",
    "            self.out = self.prod\n",
    "            return self.out\n",
    "        elif(self.error_type == \"InnerDot\"):\n",
    "            #No Error Model first\n",
    "            self.clamped_w=binarize(self.w)\n",
    "            self.clamped_x=binarize(x)\n",
    "            gamma_x = tf.math.maximum(K.abs(tf.math.reduce_min(x)),K.abs(tf.math.reduce_max(x)))\n",
    "            #self.prod = K.dot(x,self.clamped_w)\n",
    "            self.pop = (K.dot(self.clamped_x,self.clamped_w)+self.n_in)/2\n",
    "\n",
    "            #Add Error \n",
    "            #Change operations to popcount\n",
    "#             pop_constraint_gamma = (self.gamma+self.n_in)/2\n",
    "#             self.pop = (self.prod+self.n_in)/2\n",
    "            self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "                                                 scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "            self.samps = self.dist.sample(1)\n",
    "#             print(self.samps.shape)\n",
    "#             tf.print(pop_constraint_gamma)\n",
    "            self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "#             tf.print(self.samps)\n",
    "#             print(self.samps.shape)\n",
    "            self.out = K.abs(self.gamma_w)*gamma_x*(2*self.samps-self.n_in)\n",
    "            return self.out\n",
    "        \n",
    "        \n",
    "        \n",
    "        #No Batch Norm\n",
    "#         if(self.error_type == \"NoError\"):\n",
    "#             self.clamped_w=binarize(self.w)\n",
    "#             self.prod=K.dot(x,self.clamped_w)-self.gamma\n",
    "#             self.out = binarize(self.prod)\n",
    "#             return self.out\n",
    "#         elif(self.error_type == \"Flip\"):\n",
    "#             self.clamped_w=binarize(self.w)*((2*tf.cast(tf.random.uniform(self.w.shape) > self.p, tf.float32)) - 1)\n",
    "#             self.prod=K.dot(x,self.clamped_w)-self.gamma\n",
    "#             self.out = binarize(self.prod)\n",
    "#             return self.out\n",
    "#         elif(self.error_type == \"InnerDot\"):\n",
    "#             #No Error Model first\n",
    "#             self.clamped_w=binarize(self.w)\n",
    "#             self.prod = K.dot(x,self.clamped_w)\n",
    "            \n",
    "#             #Add Error \n",
    "#             #Change operations to popcount\n",
    "#             pop_constraint_gamma = (self.gamma+self.n_in)/2\n",
    "#             self.pop = (self.prod+self.n_in)/2\n",
    "#             self.dist = tfp.distributions.Normal(loc = self.pop * self.ul + (self.n_in - self.pop) * self.uh,\n",
    "#                                                  scale= tf.math.sqrt(self.pop * (self.sigl**2) + (self.n_in - self.pop)*(self.sigh**2)))\n",
    "#             self.samps = self.dist.sample(1)\n",
    "# #             print(self.samps.shape)\n",
    "#             tf.print(pop_constraint_gamma)\n",
    "#             self.samps = tf.squeeze(self.samps, axis = 0)\n",
    "#             tf.print(self.samps)\n",
    "# #             print(self.samps.shape)\n",
    "#             self.out = binarize(self.samps-pop_constraint_gamma)\n",
    "#             return self.out\n",
    "        \n",
    "    \n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (self.n_in, self.n_out)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_in': self.n_in,\n",
    "            'n_out': self.n_out,\n",
    "            'error_type': self.error_type,\n",
    "            'error_dict': self.error_dict\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9ca1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Code\n",
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p \n",
    "\n",
    "# 2 : InnerProd error\n",
    "# error_type = \"NoError\"\n",
    "# error_dict = {}\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "# error_type = \"Flip\"\n",
    "# error_dict = {\"p\": p }\n",
    "\n",
    "#InnerProd paramslayer.call([0,0,0])\n",
    "ul = 1\n",
    "uh = 0.01\n",
    "sigl = 0.01\n",
    "sigh = 0.01\n",
    "\n",
    "error_type = \"InnerDot\"\n",
    "error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "\n",
    "layer = binary_dense_error_var(n_in=700,n_out=256,input_shape=[3],error_type = error_type, error_dict = error_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8db1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Construction\n",
    "\n",
    "def make_MNIST_model(error_type, p = 0.1, ul = 1, sigl = 0.01, uh=0.01, sigh = 0.01 ):\n",
    "    if(error_type == 0):\n",
    "        #No Error\n",
    "        error_type = \"NoError\"\n",
    "        error_dict = {}\n",
    "    elif(error_type == 1):\n",
    "        #Flip Error\n",
    "        error_type = \"Flip\"\n",
    "        error_dict = {\"p\": p }\n",
    "    elif(error_type == 2):\n",
    "        #InnerDot error \n",
    "        error_type = \"InnerDot\"\n",
    "        error_dict = {\"ul\" : ul , \"sigl\" : sigl, \"uh\" : uh, \"sigh\" : sigh}\n",
    "        \n",
    "    #Fixed Parameters \n",
    "    batch_norm_eps=1e-4\n",
    "    batch_norm_alpha=0.1#(this is same as momentum)\n",
    "    \n",
    "    #Model Def\n",
    "    #5 Binary dense layers,final layer is softmax\n",
    "    model=Sequential()\n",
    "    #First Layer Group\n",
    "    model.add(binary_dense_error_var(\n",
    "        n_in=784,n_out=256,input_shape=[784],error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Sign_layer(levels=1))\n",
    "\n",
    "    #Subsequent Dense Layers\n",
    "    #Change Layer Sizes\n",
    "#     layer_sizes = [1024,1024]\n",
    "    layer_sizes = [256,256,256]\n",
    "    for i in range(len(layer_sizes)):\n",
    "        model.add(binary_dense_error_var(\n",
    "            n_in=int(model.output.get_shape()[1]),n_out=layer_sizes[i],error_type = error_type, error_dict = error_dict))\n",
    "        model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "        model.add(Sign_layer(levels=1))\n",
    "\n",
    "    \n",
    "    #Final Layer\n",
    "    model.add(binary_dense_error_var(\n",
    "            n_in=int(model.output.get_shape()[1]),n_out=10,error_type = error_type, error_dict = error_dict))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the following is a workaround so that the model weights can be saved\n",
    "    # https://github.com/tensorflow/tensorflow/issues/46871\n",
    "    j = 0\n",
    "    for w in model.weights:\n",
    "        w._handle_name = 'model_' + str(j) + w.name\n",
    "        j = j + 1\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8117e",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1879470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "binary_dense_error_var_20 (b (None, 256)               200705    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_16 (Sign_layer)   (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_21 (b (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_17 (Sign_layer)   (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_22 (b (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_18 (Sign_layer)   (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_23 (b (None, 256)               65537     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "sign_layer_19 (Sign_layer)   (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "binary_dense_error_var_24 (b (None, 10)                2561      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 404,017\n",
      "Trainable params: 401,949\n",
      "Non-trainable params: 2,068\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "error_type = 2\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "\n",
    "#InnerProd params\n",
    "ul = 1 #l-low means bits match\n",
    "uh = 0.1\n",
    "sigl = 0.01\n",
    "sigh = 0.01\n",
    "\n",
    "#Make Model\n",
    "model = make_MNIST_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13ab77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1080/1080 - 8s - loss: 1.2929 - accuracy: 0.5614 - val_loss: 1.2386 - val_accuracy: 0.5762\n",
      "Epoch 2/10\n",
      "1080/1080 - 5s - loss: 1.0170 - accuracy: 0.6586 - val_loss: 0.8043 - val_accuracy: 0.7297\n",
      "Epoch 3/10\n",
      "1080/1080 - 5s - loss: 0.7902 - accuracy: 0.7361 - val_loss: 0.7695 - val_accuracy: 0.7448\n",
      "Epoch 4/10\n",
      "1080/1080 - 6s - loss: 0.7917 - accuracy: 0.7347 - val_loss: 0.7533 - val_accuracy: 0.7480\n",
      "Epoch 5/10\n",
      "1080/1080 - 5s - loss: 0.7843 - accuracy: 0.7404 - val_loss: 0.7845 - val_accuracy: 0.7402\n",
      "Epoch 6/10\n",
      "1080/1080 - 5s - loss: 0.7863 - accuracy: 0.7378 - val_loss: 0.8164 - val_accuracy: 0.7292\n",
      "Epoch 7/10\n",
      "1080/1080 - 5s - loss: 0.7839 - accuracy: 0.7383 - val_loss: 0.7453 - val_accuracy: 0.7533\n",
      "Epoch 8/10\n",
      "1080/1080 - 5s - loss: 0.7869 - accuracy: 0.7396 - val_loss: 0.7663 - val_accuracy: 0.7463\n",
      "Epoch 9/10\n",
      "1080/1080 - 5s - loss: 0.8205 - accuracy: 0.7266 - val_loss: 0.8979 - val_accuracy: 0.6967\n",
      "Epoch 10/10\n",
      "1080/1080 - 6s - loss: 0.9067 - accuracy: 0.6935 - val_loss: 0.8610 - val_accuracy: 0.7072\n"
     ]
    }
   ],
   "source": [
    "#Training Script\n",
    "    \n",
    "#Training parameters\n",
    "batch_size=50\n",
    "epochs=100\n",
    "\n",
    "#Makes Model Name\n",
    "if(error_type == 0):\n",
    "    #No Error\n",
    "    model_name = \"error_{0}\".format(error_type)\n",
    "elif(error_type == 1):\n",
    "    model_name = \"error_{0}_p_{1}\".format(error_type,p)\n",
    "elif(error_type == 2):\n",
    "    #InnerDot error \n",
    "    model_name = \"error_{0}_ul_{1}_uh_{2}_sigl_{3}_sigh_{4}\".format(error_type,ul,uh,sigl,sigh)\n",
    "\n",
    "if not(os.path.exists('models')):\n",
    "    os.mkdir('models')\n",
    "if not(os.path.exists('models/'+model_name)):\n",
    "    os.mkdir('models/'+model_name)\n",
    "    \n",
    "    \n",
    "# #gather all binary dense and binary convolution layers:\n",
    "# binary_layers=[]\n",
    "# for l in model.layers:\n",
    "#     if isinstance(l,binary_dense) or isinstance(l,binary_conv):\n",
    "#         binary_layers.append(l)\n",
    "\n",
    "#Train\n",
    "lr=0.01\n",
    "opt = keras.optimizers.Adam(learning_rate=lr,decay=1e-6)#SGD(lr=lr,momentum=0.9,decay=1e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "\n",
    "\n",
    "weights_path='models/'+model_name+'.h5'\n",
    "cback=keras.callbacks.ModelCheckpoint(weights_path, monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n",
    "\n",
    "\n",
    "#Training\n",
    "if keras.__version__[0]=='2':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,epochs=epochs,callbacks=[cback])\n",
    "if keras.__version__[0]=='1':\n",
    "    history=model.fit(X_train, y_train,batch_size=batch_size,validation_data=(X_val, y_val), verbose=2,nb_epoch=epochs,callbacks=[cback])\n",
    "\n",
    "#Save history    \n",
    "dic={'hard':history.history}\n",
    "foo=open('models/'+model_name+'.pkl','wb')\n",
    "pickle.dump(dic,foo)\n",
    "foo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb271c57",
   "metadata": {},
   "source": [
    "# Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b30a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Based on Error Type\n",
    "#Model Parameters\n",
    "# 0 : No error, params ignored\n",
    "# 1 : Flip error, only p used\n",
    "# 2 : InnerProd error\n",
    "error_type = 0\n",
    "\n",
    "#Flip Probabiliy\n",
    "p=0.1\n",
    "\n",
    "#InnerProd params\n",
    "ul = 1\n",
    "uh = 0.01\n",
    "sigl = 0.001\n",
    "sigh = 0.001\n",
    "\n",
    "weights_path = \"\" #To Specify\n",
    "\n",
    "if(error_type == 0)\n",
    "    #No Error\n",
    "    model=make_MNIST_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "        \n",
    "    #Load Weights\n",
    "    model.load_weights(weights_path)0.001\n",
    "    \n",
    "    score=model.evaluate(X_test,y_test,verbose=0)\n",
    "    print(\"No Error, test loss was %0.4f, test accuracy was %0.4f\"%(p,score[0],score[1]))\n",
    "\n",
    "elif(errro_type == 1):\n",
    "    #Flip Error\n",
    "    probs = np.logspace(-4, -1, 20)\n",
    "    \n",
    "    acc_1 = []\n",
    "    for p in probs:\n",
    "        \n",
    "        model=make_MNIST_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = sigh )\n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        print(\"With p= %d, test loss was %0.4f, test accuracy was %0.4f\"%(p,score[0],score[1]))\n",
    "        acc_1.append(score[1])\n",
    "    \n",
    "elif(error_type == 2)\n",
    "    #InnerDot Error\n",
    "    error_type = \"InnerDot\"\n",
    "    \n",
    "    ratio_l_h = np.linspace(1,10,20)\n",
    "    acc_2 = []\n",
    "    for ratio in ratio_l_h:\n",
    "        \n",
    "        model = make_MNIST_model(error_type, p = p, ul = ul, sigl = sigl, uh=uh, sigh = ratio*sigl )\n",
    "        \n",
    "        #Load Weights\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        score=model.evaluate(X_test,y_test,verbose=0)\n",
    "        print(\"With ratio %d, test loss was %0.4f, test accuracy was %0.4f\"%(ratio,score[0],score[1]))\n",
    "        acc_2.append(score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
